import os
import json
from dotenv import load_dotenv
from openai import OpenAI
from langchain_community.embeddings.dashscope import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph
from typing_extensions import List, TypedDict
from langchain_core.documents import Document
import base64
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException, File, UploadFile

# å¸¸é‡å®šä¹‰
INDEX_DIR = "anime_faiss_index"  # ç´¢å¼•å­˜å‚¨ç›®å½•
DATA_FILE = 'anime_data.json'  # åŸå§‹æ•°æ®æ–‡ä»¶

class ArkText2ImageRequest(BaseModel):
    prompt: str

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    load_dotenv()
except ImportError:
    pass

# è±†åŒ…APIè®¾ç½® - ç»Ÿä¸€ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹
ARK_API_KEY = os.getenv("ARK_API_KEY", "014a18da-8e40-4de2-8139-030cf822a3e3")
MULTIMODAL_MODEL = "doubao-seed-1-6-250615"  # ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    api_key=ARK_API_KEY,
    base_url="https://ark.cn-beijing.volces.com/api/v3"
)

os.environ["DASHSCOPE_API_KEY"] = "sk-9f33ea712a164d2bb40059c101467eb0"
embeddings = DashScopeEmbeddings(model="text-embedding-v1")

def load_anime_data():
    """åŠ è½½åŸå§‹åŠ¨æ¼«æ•°æ®"""
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_documents(anime_data):
    """å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ–‡æ¡£å¯¹è±¡"""
    all_docs = []
    MAX_EMBED_LENGTH = 2048
    for anime in anime_data:
        studios = [studio['name'].lower() for studio in anime['studios']['nodes']]

        # æå–å›¾ç‰‡URLï¼Œå¸¸è§å­—æ®µ coverImage æˆ– image_url
        image_url = None
        if 'coverImage' in anime and 'large' in anime['coverImage']:
            image_url = anime['coverImage']['large']
        elif 'image_url' in anime:
            image_url = anime['image_url']

        content_parts = [
            f"Title: {anime['title']['english'] or anime['title']['romaji']}",
            f"Native Title: {anime['title']['native']}",
            f"Description: {anime['description']}",
            f"Genres: {', '.join(anime['genres'])}",
            f"Tags: {', '.join([tag['name'] for tag in anime['tags']])}",
            f"Average Score: {anime['averageScore']}",
            f"Popularity: {anime['popularity']}",
            f"Release Date: {anime['startDate']['year']}-{anime['startDate']['month']}",
            f"Episodes: {anime['episodes']}",
            f"Duration: {anime['duration']} minutes",
            f"Studios: {', '.join(studios)}",
            f"Image URL: {image_url if image_url else 'N/A'}"
        ]
        page_content = "\n".join(content_parts)
        if len(page_content) > MAX_EMBED_LENGTH:
            page_content = page_content[:MAX_EMBED_LENGTH]
        metadata = {
            "title": anime['title']['english'] or anime['title']['romaji'],
            "genres": anime['genres'],
            "tags": [tag['name'] for tag in anime['tags']],
            "score": anime['averageScore'],
            "popularity": anime['popularity'],
            "year": anime['startDate']['year'],
            "studios": studios,
            "image_url": image_url
        }
        all_docs.append(Document(page_content=page_content, metadata=metadata))
    return all_docs

def create_search_metadata(docs):
    """ä¸ºæ–‡æ¡£åˆ›å»ºæœç´¢å…ƒæ•°æ®"""
    for doc in docs:
        metadata = doc.metadata
        metadata["search_keywords"] = (
            f"{metadata['title']} "
            f"{'/'.join(metadata['genres'])} "
            f"{'/'.join(metadata['tags'])} "
            f"{'/'.join(metadata['studios'])} "
            f"{metadata['year']}"
        ).lower()

        metadata["clean_content"] = (
            f"æ ‡é¢˜: {metadata['title']}\n"
            f"ç±»å‹: {', '.join(metadata['genres'])}\n"
            f"æ ‡ç­¾: {', '.join(metadata['tags'])}\n"
            f"å·¥ä½œå®¤: {', '.join(metadata['studios'])}\n"
            f"å¹´ä»½: {metadata['year']}\n"
            f"è¯„åˆ†: {metadata['score']}\n"
            f"ç®€ä»‹: {doc.page_content.split('Description: ')[1].split('\n')[0] if 'Description: ' in doc.page_content else ''}"
        )
    return docs

def get_vector_store():
    """è·å–æˆ–åˆ›å»ºæŒä¹…åŒ–å‘é‡å­˜å‚¨"""
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    if os.path.exists(INDEX_DIR) and os.listdir(INDEX_DIR):
        print(f"åŠ è½½ç°æœ‰ç´¢å¼•: {INDEX_DIR}")
        return FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)

    print("åˆ›å»ºæ–°ç´¢å¼•...")
    # åŠ è½½å¹¶å¤„ç†æ•°æ®
    anime_data = load_anime_data()
    anime_docs = create_documents(anime_data)
    processed_docs = create_search_metadata(anime_docs)

    # åˆ›å»ºFAISSç´¢å¼•
    vector_store = FAISS.from_documents(processed_docs, embeddings)

    # ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°
    vector_store.save_local(INDEX_DIR)
    print(f"å·²ä¿å­˜ç´¢å¼•åˆ°: {INDEX_DIR}")
    return vector_store

def build_anime_recommender(vector_store):
    """æ„å»ºæ¨èç³»ç»Ÿ"""
    template = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„åŠ¨æ¼«çˆ±å¥½è€…å…¼æ¨èä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å’Œä»¥ä¸‹åŠ¨æ¼«ä¿¡æ¯ï¼Œä»¥åŠ¨æ¼«è¿·çš„è§†è§’ä¸ºç”¨æˆ·æä¾›æ¨èå’Œç›¸å…³ä¿¡æ¯ï¼š

        {context}

        ç”¨æˆ·è¯¢é—®ï¼š{question}

        è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„å’Œé£æ ¼å›å¤ï¼š

        ### ğŸŒ æ¨èç†ç”±ï¼ˆåŠ¨æ¼«è¿·è§†è§’ï¼‰
        ç”¨çƒ­æƒ…æ´‹æº¢çš„è¯­æ°”è§£é‡Šä¸ºä»€ä¹ˆæ¨èè¿™äº›åŠ¨æ¼«ï¼Œå¯ä»¥åŒ…å«ï¼š
        - åˆ¶ä½œäº®ç‚¹ï¼ˆå¦‚ufotableçš„ä½œç”»ã€WIT Studioçš„ç«‹ä½“æœºåŠ¨ç­‰ï¼‰
        - åœ¨åŠ¨æ¼«åœˆå†…çš„å£ç¢‘å’Œåœ°ä½
        - ä¸ªäººè§‚æ„Ÿè¯„ä»·ï¼ˆä»¥åŠ¨æ¼«è¿·è§’åº¦ï¼‰
        - é€‚åˆä»€ä¹ˆç±»å‹çš„è§‚ä¼—

        ### ğŸ”¥ æ¨èä½œå“è¯¦æƒ…
        ä¸ºæ¯ä¸ªæ¨èä½œå“æä¾›ä¸°å¯Œä¿¡æ¯ï¼š
        1. ã€Šä½œå“åç§°ã€‹ (ç±»å‹)
        - åˆ¶ä½œå…¬å¸ï¼šXXX
        - æ’­å‡ºæ—¶é—´ï¼šXXXXå¹´
        - è±†ç“£/MyAnimeListè¯„åˆ†ï¼šX.X
        - æ ¸å¿ƒçœ‹ç‚¹ï¼š2-3ä¸ªæœ€çªå‡ºçš„äº®ç‚¹
        - ç»å…¸åœºæ™¯/ååœºé¢ï¼šç®€è¦æè¿°1-2ä¸ª
        - è§‚çœ‹å»ºè®®ï¼šæœ€ä½³è§‚çœ‹å¹³å°/æœ€ä½³è§‚çœ‹é¡ºåºï¼ˆå¦‚æœ‰ç³»åˆ—ä½œï¼‰

        2. ã€Šä½œå“åç§°ã€‹ (ç±»å‹)
        [åŒä¸Šæ ¼å¼]

        ### ğŸ’¡ æ·±åº¦å»¶ä¼¸
        æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å»¶ä¼¸ç›¸å…³ä¿¡æ¯ï¼š
        - å¦‚æœæ˜¯ç‰¹å®šå…¬å¸ï¼šä»‹ç»è¯¥å…¬å¸é£æ ¼/ä»£è¡¨ä½œå“/ä¸šç•Œåœ°ä½
        - å¦‚æœæ˜¯ç±»å‹/é¢˜æï¼šä»‹ç»è¯¥ç±»å‹å‘å±•å²/ç»å…¸ä½œå“
        - å¦‚æœæ˜¯å¹´ä»£ï¼šä»‹ç»è¯¥å¹´ä»£åŠ¨ç”»ç‰¹ç‚¹
        - ç›¸å…³åˆ¶ä½œäººå‘˜ï¼ˆç›‘ç£/è„šæœ¬/éŸ³ä¹ç­‰ï¼‰çš„å…¶ä»–çŸ¥åä½œå“

        ### ğŸŒŸ ä¸ªæ€§åŒ–å»ºè®®
        - è§‚çœ‹é¡ºåºå»ºè®®ï¼ˆå¦‚æœ‰ç³»åˆ—ï¼‰
        - ç±»ä¼¼é£æ ¼çš„å…¶ä»–ä½œå“
        - è§‚çœ‹å‰çš„å°è´´å£«
        - å¯èƒ½ä¼šå–œæ¬¢çš„å…¶ä»–å…ƒç´ 

        è¯·ç”¨æ´»æ³¼ä½†ä¸“ä¸šçš„åŠ¨æ¼«çˆ±å¥½è€…è¯­æ°”ï¼Œé€‚å½“ä½¿ç”¨emojiå’ŒåŠ¨æ¼«åœˆæœ¯è¯­ï¼Œä½†ä¿æŒä¿¡æ¯å‡†ç¡®ã€‚å›å¤è¯­è¨€ä¸ç”¨æˆ·æŸ¥è¯¢ä¸€è‡´ã€‚"""

    prompt = PromptTemplate.from_template(template)

    class State(TypedDict):
        question: str
        context: List[Document]
        answer: str

    def retrieve(state: State):
        query = state["question"].lower()
        studio_keywords = ["studio", "å…¬å¸", "åˆ¶ä½œ"]

        if any(keyword in query for keyword in studio_keywords):
            # å°è¯•æå–å·¥ä½œå®¤åç§°
            studio_name = None
            for word in query.split():
                if word.lower() in ["ufotable", "é£ç¢Ÿç¤¾"]:
                    studio_name = "ufotable"
                    break

            if studio_name:
                # ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
                all_docs = vector_store.docstore._dict.values()
                studio_docs = [
                    doc for doc in all_docs
                    if studio_name in doc.metadata.get("studios", [])
                ]
                return {"context": studio_docs[:5]}

        # æ™®é€šç›¸ä¼¼åº¦æœç´¢
        retrieved_docs = vector_store.similarity_search(state["question"], k=5)
        return {"context": retrieved_docs}

    def generate(state: State):
        if not state["context"]:
            return {"answer": "æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆè¦æ±‚çš„åŠ¨æ¼«ä½œå“ï¼Œè¯·å°è¯•å…¶ä»–æŸ¥è¯¢æ¡ä»¶ã€‚"}

        docs_content = "\n\n".join(doc.page_content for doc in state["context"])
        prompt_value = prompt.invoke({
            "question": state["question"],
            "context": docs_content
        })

        messages = prompt_value.to_messages()
        messages.insert(0, SystemMessage(content="è¯·ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œä¸è¦é‡å¤ä»»ä½•éƒ¨åˆ†ã€‚"))

        # è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„æ¶ˆæ¯
        openai_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                openai_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                openai_messages.append({"role": "user", "content": msg.content})

        # ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆæ¨è
        try:
            # ä½¿ç”¨éæµå¼å“åº”ï¼Œé¿å…ç”Ÿæˆå™¨é—®é¢˜
            response = client.chat.completions.create(
                model=MULTIMODAL_MODEL,
                messages=openai_messages,
                max_tokens=2000,
                temperature=0.7,

            )
            return {"answer": response.choices[0].message.content}
        except Exception as e:
            return {"answer": f"APIè°ƒç”¨é”™è¯¯: {str(e)}"}

    # å®šä¹‰ENDèŠ‚ç‚¹å‡½æ•°
    def end_node(state: State):
        return state

    graph_builder = StateGraph(State)
    graph_builder.add_node("retrieve", retrieve)
    graph_builder.add_node("generate", generate)
    graph_builder.set_entry_point("retrieve")
    graph_builder.add_edge("retrieve", "generate")

    END = "END"
    graph_builder.add_node(END, end_node)
    graph_builder.add_edge("generate", END)

    return graph_builder.compile()

# åˆå§‹åŒ–ç³»ç»Ÿ
print("æ­£åœ¨åˆå§‹åŒ–åŠ¨æ¼«æ¨èç³»ç»Ÿ...")
vector_store = get_vector_store()  # è·å–æŒä¹…åŒ–å‘é‡å­˜å‚¨
anime_recommender = build_anime_recommender(vector_store)
print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

def recognize_anime_image(image_path):
    """ä½¿ç”¨è±†åŒ…å¤šæ¨¡æ€æ¨¡å‹è¯†åˆ«åŠ¨æ¼«å›¾ç‰‡å†…å®¹"""
    try:
        # è¯»å–å¹¶ç¼–ç å›¾åƒ
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode("utf-8")
        
        # æ„å»ºå¤šæ¨¡æ€è¯·æ±‚ï¼ˆä¿®æ­£åçš„æ ¼å¼ï¼‰
        response = client.chat.completions.create(
            model=MULTIMODAL_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",  # æ·»åŠ typeå­—æ®µ
                            "image_url": {
                                "url": f"data:image/png;base64,{image_data}"
                            }
                        },
                        {
                            "type": "text",  # æ·»åŠ typeå­—æ®µ
                            "text": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„åŠ¨æ¼«ç›¸å…³å†…å®¹ï¼ŒåŒ…æ‹¬è§’è‰²ã€ä½œå“ã€åœºæ™¯ç­‰ã€‚"
                        }
                    ]
                }
            ],
            max_tokens=1000
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"å›¾åƒè¯†åˆ«é”™è¯¯: {str(e)}")
        return "æ— æ³•è¯†åˆ«æ­¤å›¾ç‰‡å†…å®¹"

# ä¿®æ”¹æ¨èå‡½æ•°ä»¥æ”¯æŒå¤šæ¨¡æ€è¾“å…¥
def get_anime_recommendation(query, image_path=None, user_id=None):
    # ä¸ªæ€§åŒ–æ¨èï¼šè¯»å–ç”¨æˆ·å–œæ¬¢çš„ä½œå“
    favorites_text = ""
    if user_id:
        try:
            with open("users.json", "r", encoding="utf-8") as f:
                users = json.load(f)
            user_data = users.get(user_id, {})
            favorites = user_data.get("favorites", [])
            if favorites:
                favorites_text = f"\nç”¨æˆ·å–œæ¬¢çš„ä½œå“ï¼š{', '.join(favorites)}"
        except Exception as e:
            print(f"è¯»å–ç”¨æˆ·å–œæ¬¢çš„ä½œå“å¤±è´¥: {e}")

    # å¦‚æœæœ‰å›¾åƒè¾“å…¥ï¼Œå…ˆè¯†åˆ«å›¾ç‰‡ï¼Œå†å°†è¯†åˆ«ç»“æœä¸æ–‡æœ¬æ‹¼æ¥åèµ°RAGæ¨è
    if image_path and os.path.exists(image_path):
        print(f"æ­£åœ¨è¯†åˆ«ä¸Šä¼ çš„å›¾ç‰‡: {image_path}")
        try:
            image_description = recognize_anime_image(image_path)
            print(f"å›¾ç‰‡è¯†åˆ«ç»“æœ: {image_description}")
            final_query = f"å›¾ç‰‡å†…å®¹ï¼š{image_description}\nç”¨æˆ·é—®é¢˜ï¼š{query}{favorites_text}"
            result = anime_recommender.invoke({"question": final_query})
            return result["answer"]
        except Exception as e:
            print(f"å›¾ç‰‡è¯†åˆ«å¤±è´¥: {str(e)}")
            return "å›¾ç‰‡è¯†åˆ«å¤±è´¥"
    else:
        # æ— å›¾ç‰‡æ—¶èµ°RAGæ¨è
        final_query = f"{query}{favorites_text}"
        result = anime_recommender.invoke({"question": final_query})
        return result["answer"]

def text2image_ark(request: ArkText2ImageRequest):
    """
    Arkæ–‡ç”Ÿå›¾æ¥å£
    è¯·æ±‚å‚æ•°: prompt æ–‡æœ¬æè¿°
    è¿”å›: å›¾ç‰‡URL
    """
    try:
        anime_style_hint = "ç”»é£ï¼šäºŒæ¬¡å…ƒåŠ¨æ¼«é£æ ¼ï¼Œæ˜äº®è‰²å½©ï¼Œç²¾è‡´çº¿æ¡ï¼Œå”¯ç¾èƒŒæ™¯ã€‚"
        client = OpenAI(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key=os.environ.get("api_key","cd2f80e0-0254-4d1e-86bf-593585203d5b"),
        )
        response = client.images.generate(
            model="ep-20250717143642-d5b6v",  # è¯·æ›¿æ¢ä¸ºä½ çš„Arkæ¨ç†æ¥å…¥ç‚¹ID
            prompt=f"{request.prompt}\n{anime_style_hint}",
            size="1024x1024",
            response_format="url"
        )
        url = response.data[0].url if response and response.data and response.data[0].url else None
        if url:
            return {"url": url}
        else:
            raise Exception("Arkå›¾ç‰‡ç”Ÿæˆå¤±è´¥")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Arkæ–‡ç”Ÿå›¾å¤±è´¥: {e}")
                            
if __name__ == "__main__":
    while True:
        user_input = input("\nè¯·è¾“å…¥ä½ æƒ³çœ‹çš„åŠ¨æ¼«ç›¸å…³ä¿¡æ¯ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰ï¼š\n> ")
        if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
            break

        print("\næ­£åœ¨ä¸ºä½ æ¨è...")
        recommendation = get_anime_recommendation(user_input)
        print("\næ¨èç»“æœï¼š")
        print(recommendation)
        print("\n" + "=" * 50)